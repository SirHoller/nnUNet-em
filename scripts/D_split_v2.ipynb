{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividir los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuración del entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/castiler/TFM'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar librerías:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.A_config import NNUNetConfig, DatasetType\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "random.seed(81)\n",
    "np.random.seed(81)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obetener los ficheros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = [file for file in os.listdir(NNUNetConfig().train_images_dir) if file.endswith(NNUNetConfig().TERMINATION)]\n",
    "all_ids = sorted({file_name.split(\".\")[0][:-5] for file_name in all_images})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.ndimage import label\n",
    "from tqdm import tqdm\n",
    "\n",
    "def detect_lesions(mask: nib.nifti1.Nifti1Image) -> Tuple[np.ndarray, Dict[str, List[int]]]:\n",
    "    \"\"\"Detect and label different lesions by using a pattern.\n",
    "\n",
    "    Args:\n",
    "        mask: nifti image with the mask\n",
    "\n",
    "    Returns:\n",
    "        basal_lesions_map: image with a different label (int) for each lesion\n",
    "        joint_lesions: a dict with basal and new lesion identifiers\n",
    "    \"\"\"\n",
    "    # Detecting lesions:\n",
    "    basal_lesions_map, n_basal_lesions = label((mask.get_fdata() == 1).astype(int))\n",
    "    followup_lesions_map, n_followup_lesions = label((mask.get_fdata() == 2).astype(int))\n",
    "    # Merging results:\n",
    "    new_lesion_ids = []\n",
    "    for new_lesion in range(1, n_followup_lesions + 1):\n",
    "        new_lesion_id = new_lesion + n_basal_lesions\n",
    "        basal_lesions_map[followup_lesions_map == new_lesion] = new_lesion_id\n",
    "        new_lesion_ids.append(new_lesion_id)\n",
    "    joint_lesions = {'basal': list(range(1, n_basal_lesions + 1)), 'new': new_lesion_ids}\n",
    "    return basal_lesions_map, joint_lesions\n",
    "\n",
    "def analyse_cases(ids: List[str], labels_dir: pathlib.Path):\n",
    "    \"\"\"Analyse cases with id in 'ids' and whose labels are in directory 'labels_dir'.\n",
    "\n",
    "    Args:\n",
    "        ids: list of ids of the cases to be analysed\n",
    "        labels_dir: directory where the labels of the ids are stored\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with all results.\n",
    "    \"\"\"\n",
    "    # We iterate over ids and gather all results in a list:\n",
    "    analysis_results = []\n",
    "    for case_id in tqdm(ids):\n",
    "        # We load the labels:\n",
    "        case_mask = nib.load(labels_dir / (case_id + NNUNetConfig().TERMINATION))\n",
    "        # We detect the lesions (both basal and new ones):\n",
    "        lesions_map, lesions = detect_lesions(case_mask)\n",
    "        # We compute the total, mean and median lesion size for both types of lesions:\n",
    "        total_basal_lesion_vol = (case_mask.get_fdata() == 1).sum()\n",
    "        total_new_lesion_vol = (case_mask.get_fdata() == 2).sum()\n",
    "        mean_basal_lesion_vol = np.median([(lesions_map == b_lesion).sum() for b_lesion in lesions['basal']])\n",
    "        median_basal_lesion_vol = np.mean([(lesions_map == b_lesion).sum() for b_lesion in lesions['basal']])\n",
    "        mean_new_lesion_vol = np.median([(lesions_map == b_lesion).sum() for b_lesion in lesions['new']])\n",
    "        median_new_lesion_vol = np.mean([(lesions_map == b_lesion).sum() for b_lesion in lesions['new']])\n",
    "        # And we append the results to the list, including the number of lesions:\n",
    "        case_results = {\n",
    "            \"case_id\": case_id,\n",
    "            \"n_lesions\": len(lesions['basal']) + len(lesions['new']),\n",
    "            \"n_basal_lesions\": len(lesions['basal']),\n",
    "            \"n_new_lesions\": len(lesions['new']),\n",
    "            \"mean_basal_lesion_vol\": mean_basal_lesion_vol,\n",
    "            \"median_basal_lesion_vol\": median_basal_lesion_vol,\n",
    "            \"total_basal_lesion_vol\": total_basal_lesion_vol,\n",
    "            \"mean_new_lesion_vol\": mean_new_lesion_vol,\n",
    "            \"median_new_lesion_vol\": median_new_lesion_vol,\n",
    "            \"total_new_lesion_vol\": total_new_lesion_vol\n",
    "        }\n",
    "        analysis_results.append(case_results)\n",
    "    # Finally, we return all results as a dataframe:\n",
    "    return pd.DataFrame.from_records(analysis_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/81 [00:00<?, ?it/s]/Users/castiler/miniforge3/envs/TFM/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/castiler/miniforge3/envs/TFM/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "100%|██████████| 81/81 [00:42<00:00,  1.89it/s]\n"
     ]
    }
   ],
   "source": [
    "lesions_analysis = analyse_cases(ids=all_ids, labels_dir=NNUNetConfig().train_labels_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_w_possible_NAs = [\n",
    "        'mean_basal_lesion_vol', 'median_basal_lesion_vol', 'mean_new_lesion_vol', 'median_new_lesion_vol'\n",
    "    ]\n",
    "lesions_analysis[cols_w_possible_NAs] = lesions_analysis[cols_w_possible_NAs].fillna(value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cutting into categorical:\n",
    "lesions_analysis['bl_bin'] = pd.qcut(lesions_analysis['n_basal_lesions'], 3)  # Cutting by quantiles\n",
    "lesions_analysis['nl_bin'] = pd.cut(  # Cutting by specific boundaries\n",
    "    lesions_analysis['n_new_lesions'],\n",
    "    bins=[0, 1, 5, np.inf],\n",
    "    right=False\n",
    ")\n",
    "# Combining both criteria:\n",
    "lesions_analysis['stratification_bin'] = (\n",
    "        lesions_analysis['bl_bin'].astype(str) + ' & ' + lesions_analysis['nl_bin'].astype(str)\n",
    ")\n",
    "# Unifying \"contiguous\" rare class for easier splitting:\n",
    "rare_classes = ['(39.667, 75.0] & [1.0, 5.0)', '(39.667, 75.0] & [5.0, inf)']\n",
    "lesions_analysis.loc[\n",
    "    lesions_analysis['stratification_bin'].isin(rare_classes), 'stratification_bin'\n",
    "] = ' | '.join(rare_classes)\n",
    "# Factorizing:\n",
    "lesions_analysis['stratification_class'] = pd.factorize(lesions_analysis['stratification_bin'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "train_val_ids, test_ids = train_test_split(lesions_analysis['case_id'], test_size=0.3,\n",
    "                                               stratify=lesions_analysis['stratification_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir(NNUNetConfig().test_images_dir)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "try:\n",
    "    os.mkdir(NNUNetConfig().test_labels_dir)\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "for test_case in test_ids:\n",
    "    # Images:\n",
    "    basal_image = f\"{test_case}_0000\" + NNUNetConfig().TERMINATION\n",
    "    followup_image = f\"{test_case}_0001\" + NNUNetConfig().TERMINATION\n",
    "    shutil.move(NNUNetConfig().train_images_dir / basal_image, NNUNetConfig().test_images_dir / basal_image)\n",
    "    shutil.move(NNUNetConfig().train_images_dir / followup_image, NNUNetConfig().test_images_dir / followup_image)\n",
    "    # Mask:\n",
    "    mask_image = test_case + NNUNetConfig().TERMINATION\n",
    "    shutil.move(NNUNetConfig().train_labels_dir / mask_image, NNUNetConfig().test_labels_dir / mask_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Cross-validation folds generation:\n",
    "train_stratification_bins = lesions_analysis.iloc[train_val_ids.index]['stratification_class']\n",
    "\n",
    "cv_folds = []\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "# Generation of the \"splits_final.json\" file:\n",
    "for train, val in skf.split(X=train_val_ids, y=train_stratification_bins):\n",
    "    fold = {\n",
    "        \"train\": train_val_ids.iloc[train].tolist(),\n",
    "        \"val\": train_val_ids.iloc[val].tolist()\n",
    "    }\n",
    "    cv_folds.append(fold)\n",
    "with open(NNUNetConfig().dataset_preprocessed_dir / 'splits_final.json', 'w') as f:\n",
    "    json.dump(cv_folds, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
