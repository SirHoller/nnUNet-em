{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividir los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuración del entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\mrtwe\\\\TFM'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar librerías:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.A_config import NNUNetConfig, DatasetType\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "random.seed(81)\n",
    "np.random.seed(81)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obetener los ficheros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = os.listdir(NNUNetConfig().train_images_dir)\n",
    "train_ids = sorted({file_name.split(\".\")[0][:-5] for file_name in train_images})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def flood_fill(label_data, basal_lesions_map, start_coords, lesion_id):\n",
    "    \"\"\"Perform a flood fill to label a lesion.\"\"\"\n",
    "    stack = [start_coords]\n",
    "    while stack:\n",
    "        x, y, z = stack.pop()\n",
    "        if (0 <= x < label_data.shape[0] and\n",
    "            0 <= y < label_data.shape[1] and\n",
    "            0 <= z < label_data.shape[2] and\n",
    "            label_data[x, y, z] > 0 and\n",
    "            basal_lesions_map[x, y, z] == 0):\n",
    "            basal_lesions_map[x, y, z] = lesion_id\n",
    "            stack.extend([(x+1, y, z), (x-1, y, z), \n",
    "                          (x, y+1, z), (x, y-1, z), \n",
    "                          (x, y, z+1), (x, y, z-1)])\n",
    "\n",
    "def detectar_lesiones(label):\n",
    "    \"\"\"Detect and label different lesions using a pattern.\n",
    "\n",
    "    Args:\n",
    "        label: nifti image with the mask.\n",
    "\n",
    "    Returns:\n",
    "        basal_lesions_map: Image with a unique label (int) for each lesion.\n",
    "        joint_lesions: A dictionary with basal and new lesion identifiers.\n",
    "    \"\"\"\n",
    "    # Convert label data to a numpy array\n",
    "    label_data = label.get_fdata()\n",
    "\n",
    "    # Initialize basal lesions map and lesion dictionary\n",
    "    basal_lesions_map = np.zeros_like(label_data, dtype=int)\n",
    "    joint_lesions = {'basal': [], 'new': []}\n",
    "\n",
    "    # Detect lesions and assign unique IDs\n",
    "    lesion_id = 1\n",
    "    for coords in np.argwhere(label_data > 0):  # Iterate over all non-zero voxels\n",
    "        x, y, z = coords\n",
    "        if basal_lesions_map[x, y, z] == 0:  # If not already labeled\n",
    "            flood_fill(label_data, basal_lesions_map, (x, y, z), lesion_id)\n",
    "            joint_lesions['basal'].append(lesion_id)\n",
    "            lesion_id += 1\n",
    "\n",
    "    return basal_lesions_map, joint_lesions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[[0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0]]]), {'basal': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30], 'new': []})\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import label as label_regions\n",
    "\n",
    "def detectar_lesiones(nifti_image):\n",
    "    \"\"\"Detect and label different lesions using a pattern.\n",
    "\n",
    "    Args:\n",
    "        nifti_image: nifti image object containing the mask.\n",
    "\n",
    "    Returns:\n",
    "        basal_lesions_map: Image with a unique label (int) for each lesion.\n",
    "        joint_lesions: A dictionary with basal and new lesion identifiers.\n",
    "    \"\"\"\n",
    "    # Convert the nifti image data to a numpy array\n",
    "    label_data = nifti_image.get_fdata()\n",
    "\n",
    "    # Detect connected components (lesions)\n",
    "    structure = np.ones((3, 3, 3))  # Define connectivity (26-connectivity)\n",
    "    basal_lesions_map, num_features = label_regions(label_data > 0, structure=structure)\n",
    "\n",
    "    # Create a dictionary of lesion IDs\n",
    "    joint_lesions = {'basal': list(range(1, num_features + 1)), 'new': []}\n",
    "\n",
    "    return basal_lesions_map, joint_lesions\n",
    "\n",
    "for case_id in train_ids:\n",
    "    # obtener la información del label\n",
    "    label_path = os.path.join(NNUNetConfig().train_labels_dir, case_id + NNUNetConfig().TERMINATION)\n",
    "    label = nib.load(label_path)\n",
    "    info_lesiones = detectar_lesiones(label)\n",
    "    print(info_lesiones)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import label as label_regions, binary_dilation, center_of_mass\n",
    "\n",
    "def obtener_estadisticas_lesiones(nifti_image, case_id):\n",
    "    \"\"\"\n",
    "    Detecta lesiones y calcula estadísticas sobre ellas.\n",
    "\n",
    "    Args:\n",
    "        nifti_image: Imagen NIfTI que contiene las etiquetas (1=basal, 2=nuevas).\n",
    "        case_id: Identificador único del caso.\n",
    "\n",
    "    Returns:\n",
    "        dict: Diccionario con las estadísticas calculadas.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calcular volumen de un voxel\n",
    "    voxel_volume = np.prod(nifti_image.header.get_zooms())\n",
    "    \n",
    "    # Convertir datos a array numpy\n",
    "    label_data = nifti_image.get_fdata()\n",
    "    \n",
    "    # Detectar y etiquetar lesiones basales y nuevas\n",
    "    basal_map, n_basal = label_regions(label_data == 1)\n",
    "    new_map, n_new = label_regions(label_data == 2)\n",
    "    \n",
    "    # Inicializar listas para volúmenes y relaciones superficie-volumen\n",
    "    basal_volumes = []\n",
    "    basal_surface_volume_ratios = []\n",
    "    basal_centroids = []\n",
    "    \n",
    "    for lesion_id in range(1, n_basal + 1):\n",
    "        lesion_voxels = basal_map == lesion_id\n",
    "        basal_volumes.append(np.sum(lesion_voxels) * voxel_volume)\n",
    "        # Calcular superficie\n",
    "        surface_voxels = np.sum(binary_dilation(lesion_voxels) & ~lesion_voxels)\n",
    "        basal_surface_volume_ratios.append(surface_voxels / basal_volumes[-1] if basal_volumes[-1] > 0 else 0)\n",
    "        # Calcular centroide\n",
    "        basal_centroids.append(center_of_mass(label_data, basal_map, lesion_id))\n",
    "    \n",
    "    new_volumes = []\n",
    "    new_surface_volume_ratios = []\n",
    "    new_centroids = []\n",
    "    \n",
    "    for lesion_id in range(1, n_new + 1):\n",
    "        lesion_voxels = new_map == lesion_id\n",
    "        new_volumes.append(np.sum(lesion_voxels) * voxel_volume)\n",
    "        # Calcular superficie\n",
    "        surface_voxels = np.sum(binary_dilation(lesion_voxels) & ~lesion_voxels)\n",
    "        new_surface_volume_ratios.append(surface_voxels / new_volumes[-1] if new_volumes[-1] > 0 else 0)\n",
    "        # Calcular centroide\n",
    "        new_centroids.append(center_of_mass(label_data, new_map, lesion_id))\n",
    "    \n",
    "    # Calcular estadísticas\n",
    "    result = {\n",
    "        \"case_id\": case_id,\n",
    "        \"n_lesions\": n_basal + n_new,\n",
    "        \"n_basal_lesions\": n_basal,\n",
    "        \"n_new_lesions\": n_new,\n",
    "        \"mean_basal_lesion_vol\": np.mean(basal_volumes) if basal_volumes else 0,\n",
    "        \"median_basal_lesion_vol\": np.median(basal_volumes) if basal_volumes else 0,\n",
    "        \"std_basal_lesion_vol\": np.std(basal_volumes) if basal_volumes else 0,\n",
    "        \"percentile25_basal_lesion_vol\": np.percentile(basal_volumes, 25) if basal_volumes else 0,\n",
    "        \"percentile75_basal_lesion_vol\": np.percentile(basal_volumes, 75) if basal_volumes else 0,\n",
    "        \"total_basal_lesion_vol\": np.sum(basal_volumes),\n",
    "        \"mean_basal_surface_volume_ratio\": np.mean(basal_surface_volume_ratios) if basal_surface_volume_ratios else 0,\n",
    "        \"largest_basal_lesion_vol\": max(basal_volumes) if basal_volumes else 0,\n",
    "        \"mean_new_lesion_vol\": np.mean(new_volumes) if new_volumes else 0,\n",
    "        \"median_new_lesion_vol\": np.median(new_volumes) if new_volumes else 0,\n",
    "        \"std_new_lesion_vol\": np.std(new_volumes) if new_volumes else 0,\n",
    "        \"percentile25_new_lesion_vol\": np.percentile(new_volumes, 25) if new_volumes else 0,\n",
    "        \"percentile75_new_lesion_vol\": np.percentile(new_volumes, 75) if new_volumes else 0,\n",
    "        \"total_new_lesion_vol\": np.sum(new_volumes),\n",
    "        \"mean_new_surface_volume_ratio\": np.mean(new_surface_volume_ratios) if new_surface_volume_ratios else 0,\n",
    "        \"largest_new_lesion_vol\": max(new_volumes) if new_volumes else 0,\n",
    "        \"ratio_basal_new_lesions\": (n_basal / n_new) if n_new > 0 else np.inf,\n",
    "        \"basal_centroids\": basal_centroids,\n",
    "        \"new_centroids\": new_centroids,\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "resultados = []\n",
    "for case_id in train_ids:\n",
    "    # Obtener la información del label\n",
    "    label_path = os.path.join(NNUNetConfig().train_labels_dir, case_id + NNUNetConfig().TERMINATION)\n",
    "    label = nib.load(label_path)\n",
    "    info_lesiones = obtener_estadisticas_lesiones(label, case_id)\n",
    "    resultados.append(info_lesiones)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'case_id': 'FIS_001_01',\n",
       " 'n_lesions': 37,\n",
       " 'n_basal_lesions': 37,\n",
       " 'n_new_lesions': 0,\n",
       " 'mean_basal_lesion_vol': 90.0,\n",
       " 'median_basal_lesion_vol': 45.0,\n",
       " 'std_basal_lesion_vol': 105.15059984920984,\n",
       " 'percentile25_basal_lesion_vol': 22.0,\n",
       " 'percentile75_basal_lesion_vol': 139.0,\n",
       " 'total_basal_lesion_vol': 3330.0,\n",
       " 'mean_basal_surface_volume_ratio': 2.2676875301238995,\n",
       " 'largest_basal_lesion_vol': 456.0,\n",
       " 'mean_new_lesion_vol': 0,\n",
       " 'median_new_lesion_vol': 0,\n",
       " 'std_new_lesion_vol': 0,\n",
       " 'percentile25_new_lesion_vol': 0,\n",
       " 'percentile75_new_lesion_vol': 0,\n",
       " 'total_new_lesion_vol': 0.0,\n",
       " 'mean_new_surface_volume_ratio': 0,\n",
       " 'largest_new_lesion_vol': 0,\n",
       " 'ratio_basal_new_lesions': inf,\n",
       " 'basal_centroids': [(41.63333333333333,\n",
       "   126.16666666666667,\n",
       "   108.03333333333333),\n",
       "  (53.84, 83.408, 71.86666666666666),\n",
       "  (52.94930875576037, 109.95391705069125, 58.9815668202765),\n",
       "  (54.57142857142857, 81.52380952380952, 82.76190476190476),\n",
       "  (57.357142857142854, 91.53571428571429, 80.10714285714286),\n",
       "  (57.638888888888886, 122.63888888888889, 52.583333333333336),\n",
       "  (57.71052631578947, 138.60526315789474, 85.52631578947368),\n",
       "  (56.0, 94.0, 78.0),\n",
       "  (57.0, 95.0, 78.0),\n",
       "  (60.0, 136.0, 88.0),\n",
       "  (63.65271966527197, 69.5397489539749, 95.28451882845188),\n",
       "  (64.93333333333334, 56.16, 87.84),\n",
       "  (64.72, 46.92, 78.84),\n",
       "  (68.04968944099379, 57.732919254658384, 77.32919254658385),\n",
       "  (70.06976744186046, 98.88953488372093, 100.54651162790698),\n",
       "  (71.0, 47.5, 77.0),\n",
       "  (70.38461538461539, 67.61538461538461, 87.0),\n",
       "  (71.98571428571428, 114.04285714285714, 102.97857142857143),\n",
       "  (72.16546762589928, 130.50359712230215, 102.01438848920863),\n",
       "  (72.55172413793103, 38.689655172413794, 79.06896551724138),\n",
       "  (75.3695652173913, 146.6304347826087, 90.1086956521739),\n",
       "  (84.31111111111112, 153.3111111111111, 59.422222222222224),\n",
       "  (91.95454545454545, 103.27272727272727, 41.0),\n",
       "  (99.25806451612904, 90.58064516129032, 40.83870967741935),\n",
       "  (99.31818181818181, 90.0, 44.68181818181818),\n",
       "  (99.8, 87.6, 47.0),\n",
       "  (109.7089552238806, 100.03358208955224, 100.24626865671642),\n",
       "  (107.0, 94.71428571428571, 101.0),\n",
       "  (107.16326530612245, 154.30612244897958, 84.04081632653062),\n",
       "  (109.29896907216495, 48.08247422680412, 88.4639175257732),\n",
       "  (110.55696202531645, 83.9367088607595, 103.21518987341773),\n",
       "  (112.7948717948718, 114.35897435897436, 99.56410256410257),\n",
       "  (111.0, 117.0, 99.0),\n",
       "  (118.48026315789474, 67.80263157894737, 90.68201754385964),\n",
       "  (119.76991150442478, 73.46017699115045, 77.08849557522124),\n",
       "  (125.05607476635514, 122.45794392523365, 50.64485981308411),\n",
       "  (129.71428571428572, 111.66883116883118, 55.72727272727273)],\n",
       " 'new_centroids': []}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultados[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribución en Train:\n",
      "stratification_class\n",
      "0     0.135802\n",
      "10    0.111111\n",
      "3     0.074074\n",
      "7     0.074074\n",
      "8     0.074074\n",
      "11    0.074074\n",
      "19    0.074074\n",
      "9     0.074074\n",
      "5     0.061728\n",
      "4     0.049383\n",
      "15    0.049383\n",
      "6     0.024691\n",
      "2     0.024691\n",
      "16    0.024691\n",
      "12    0.012346\n",
      "17    0.012346\n",
      "14    0.012346\n",
      "13    0.012346\n",
      "1     0.012346\n",
      "18    0.012346\n",
      "Name: proportion, dtype: float64\n",
      "Distribución en Test:\n",
      "stratification_class\n",
      "0     0.138889\n",
      "10    0.111111\n",
      "9     0.083333\n",
      "8     0.083333\n",
      "7     0.055556\n",
      "11    0.055556\n",
      "19    0.055556\n",
      "3     0.055556\n",
      "5     0.055556\n",
      "6     0.027778\n",
      "17    0.027778\n",
      "1     0.027778\n",
      "4     0.027778\n",
      "14    0.027778\n",
      "18    0.027778\n",
      "15    0.027778\n",
      "2     0.027778\n",
      "12    0.027778\n",
      "16    0.027778\n",
      "13    0.027778\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Convertir resultados a un DataFrame\n",
    "lesions_analysis = pd.DataFrame(resultados)\n",
    "\n",
    "# Estratificación inicial\n",
    "# 1. Discretizar n_basal_lesions y n_new_lesions\n",
    "lesions_analysis['bl_bin'] = pd.qcut(lesions_analysis['n_basal_lesions'], q=3, duplicates='drop')  # Por cuantiles\n",
    "lesions_analysis['nl_bin'] = pd.cut(  # Por rangos personalizados\n",
    "    lesions_analysis['n_new_lesions'], \n",
    "    bins=[0, 1, 5, np.inf], \n",
    "    right=False\n",
    ")\n",
    "\n",
    "# 2. Discretizar total_basal_lesion_vol y total_new_lesion_vol\n",
    "lesions_analysis['total_basal_vol_bin'] = pd.qcut(lesions_analysis['total_basal_lesion_vol'], q=3, duplicates='drop')\n",
    "lesions_analysis['total_new_vol_bin'] = pd.cut(\n",
    "    lesions_analysis['total_new_lesion_vol'], \n",
    "    bins=[0, 10, 50, np.inf], \n",
    "    right=False\n",
    ")\n",
    "\n",
    "# 3. Crear una clase combinada para estratificación\n",
    "lesions_analysis['stratification_bin'] = (\n",
    "    lesions_analysis['bl_bin'].astype(str) + ' & ' +\n",
    "    lesions_analysis['nl_bin'].astype(str) + ' & ' +\n",
    "    lesions_analysis['total_basal_vol_bin'].astype(str) + ' & ' +\n",
    "    lesions_analysis['total_new_vol_bin'].astype(str)\n",
    ")\n",
    "\n",
    "# 4. Factorizar la clase combinada\n",
    "lesions_analysis['stratification_class'] = pd.factorize(lesions_analysis['stratification_bin'])[0]\n",
    "\n",
    "# Manejo de clases raras\n",
    "# Contar las frecuencias de cada clase\n",
    "class_counts = lesions_analysis['stratification_class'].value_counts()\n",
    "\n",
    "# Definir un umbral mínimo (clases con menos de 2 muestras se agrupan)\n",
    "threshold = 2\n",
    "rare_classes = class_counts[class_counts < threshold].index\n",
    "\n",
    "# Reemplazar las clases raras por una categoría común\n",
    "lesions_analysis.loc[\n",
    "    lesions_analysis['stratification_class'].isin(rare_classes), \n",
    "    'stratification_class'\n",
    "] = -1  # Categoría para clases raras\n",
    "\n",
    "# Volver a verificar las clases después del agrupamiento\n",
    "lesions_analysis['stratification_class'] = pd.factorize(lesions_analysis['stratification_class'])[0]\n",
    "\n",
    "# División en Train y Test (estratificada)\n",
    "train_val_ids, test_ids = train_test_split(\n",
    "    lesions_analysis['case_id'], \n",
    "    test_size=0.3, \n",
    "    stratify=lesions_analysis['stratification_class']\n",
    ")\n",
    "\n",
    "# Verificar balance de clases en Train y Test\n",
    "train_classes = lesions_analysis.loc[lesions_analysis['case_id'].isin(train_val_ids), 'stratification_class']\n",
    "test_classes = lesions_analysis.loc[lesions_analysis['case_id'].isin(test_ids), 'stratification_class']\n",
    "\n",
    "print(f\"Distribución en Train:\\n{train_classes.value_counts(normalize=True)}\")\n",
    "print(f\"Distribución en Test:\\n{test_classes.value_counts(normalize=True)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81, 36)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_val_ids), len(test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir(NNUNetConfig().test_images_dir)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "try:\n",
    "    os.mkdir(NNUNetConfig().test_labels_dir)\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "for case_id in test_ids:\n",
    "    # Mover imágenes de test a la carpeta correspondiente, basal, follow-up y label\n",
    "    basal_image_path = os.path.join(NNUNetConfig().train_images_dir, case_id + \"_0000\" + NNUNetConfig().TERMINATION)\n",
    "    follow_up_image_path = os.path.join(NNUNetConfig().train_images_dir, case_id + \"_0001\" + NNUNetConfig().TERMINATION) \n",
    "    label_path = os.path.join(NNUNetConfig().train_labels_dir, case_id + NNUNetConfig().TERMINATION)\n",
    "    \n",
    "    shutil.move(basal_image_path, os.path.join(NNUNetConfig().test_images_dir, os.path.basename(basal_image_path)))\n",
    "    shutil.move(follow_up_image_path, os.path.join(NNUNetConfig().test_images_dir, os.path.basename(follow_up_image_path)))\n",
    "    shutil.move(label_path, os.path.join(NNUNetConfig().test_labels_dir, os.path.basename(label_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se han generado 5 pliegues para validación cruzada.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mrtwe\\miniforge3\\envs\\TFM\\lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import json\n",
    "\n",
    "# Preparar los datos para StratifiedKFold\n",
    "train_stratification_bins = lesions_analysis.loc[\n",
    "    lesions_analysis['case_id'].isin(train_val_ids),\n",
    "    'stratification_class'\n",
    "]\n",
    "train_case_ids = lesions_analysis.loc[\n",
    "    lesions_analysis['case_id'].isin(train_val_ids),\n",
    "    'case_id'\n",
    "]\n",
    "\n",
    "# Inicializar StratifiedKFold\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Generar pliegues y guardar en lista\n",
    "cv_folds = []\n",
    "for train_idx, val_idx in skf.split(X=train_case_ids, y=train_stratification_bins):\n",
    "    fold = {\n",
    "        \"train\": train_case_ids.iloc[train_idx].tolist(),\n",
    "        \"val\": train_case_ids.iloc[val_idx].tolist()\n",
    "    }\n",
    "    cv_folds.append(fold)\n",
    "\n",
    "# Guardar los pliegues en un archivo JSON\n",
    "#if is windows os\n",
    "if os.name == 'nt':\n",
    "    with open(NNUNetConfig().dataset_preprocessed_dir / \"splits_final.json\", \"w\") as f:\n",
    "        json.dump(cv_folds, f)\n",
    "else:\n",
    "    with open(NNUNetConfig().dataset_preprocessed_dir() / \"splits_final.json\", \"w\") as f:\n",
    "        json.dump(cv_folds, f)\n",
    "\n",
    "\n",
    "print(f\"Se han generado {n_splits} pliegues para validación cruzada.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
